{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGZW90mTVNfVMV47HZmtvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praburocking/transformers/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
        "# !python -m spacy download de_core_news_sm\n",
        "# !python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "khxo2YoFIxn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43a8844-a3de-44f4-8c5d-cea842132187"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 47 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 57.8 MB/s \n",
            "\u001b[?25h  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.2.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 815 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2022.6.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.3.0\n",
            "    Uninstalling en-core-web-sm-3.3.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.3.0\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AG_yyBN_NYxu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "#import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.nn import LayerNorm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=torch.rand(4,3)\n",
        "w=torch.rand(2,3)\n",
        "print(q)\n",
        "print(w)\n",
        "print((q@w.T).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u96i4AmHvAv9",
        "outputId": "3a1dec5d-e2bd-4468-e32f-05cd0451cdc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8505, 0.4121, 0.1689],\n",
            "        [0.1800, 0.8188, 0.6117],\n",
            "        [0.9316, 0.9531, 0.9734],\n",
            "        [0.9507, 0.3687, 0.8616]])\n",
            "tensor([[0.7628, 0.4848, 0.8662],\n",
            "        [0.7363, 0.4586, 0.0480]])\n",
            "torch.Size([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_interactive_notebook():\n",
        "    return __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "\n",
        "def execute_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        fn(*args)\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def __init__(self):\n",
        "        self.param_groups = [{\"lr\": 0}]\n",
        "        None\n",
        "\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        None\n",
        "\n",
        "\n",
        "class DummyScheduler:\n",
        "    def step(self):\n",
        "        None"
      ],
      "metadata": {
        "id": "eoERxGDKPK0p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  first encoderDecoder (standard one)\n",
        "  \"\"\"\n",
        "  def __init__(self,encoder,decoder,src_vocab_len,tgt_vocab_len,d_model,generator):\n",
        "    super(EncoderDecoder,self).__init__()\n",
        "    \n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=nn.Embedding(src_vocab_len,d_model)\n",
        "    self.tgt_embed=nn.Embedding(tgt_vocab_len,d_model)\n",
        "    self.pos=PositionalEncoding(d_model)\n",
        "    self.gen=generator\n",
        "    \n",
        "  def forward(self,src,src_mask,tgt,tgt_mask):\n",
        "    return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  def encode(self,src,mask_src):\n",
        "    # breakpoint()\n",
        "    return self.encoder(self.pos(self.src_embed(src)),mask_src)\n",
        "\n",
        "  def decode(self,mem,mem_mask,tgt,tgt_mask):\n",
        "    # breakpoint()\n",
        "    return self.gen(self.decoder(mem,mem_mask,self.pos(self.tgt_embed(tgt)),tgt_mask))"
      ],
      "metadata": {
        "id": "okwfYb5ePTi-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super(Generator,self).__init__()\n",
        "    self.linear=nn.Linear(d_model,vocab_size)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return log_softmax(self.linear(x),dim=-1)"
      ],
      "metadata": {
        "id": "VTWZJQPodp3E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "z_3cm87qtsRS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,N,module):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.encodeLayers=clone(module,N)\n",
        "      \n",
        "  def forward(self,x,mask):\n",
        "    for encodeLayer in self.encodeLayers:\n",
        "      x=encodeLayer(x,mask)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,N,module):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.decodeLayers=clone(module,N)\n",
        "    \n",
        "  def forward(self,mem,mem_mask,y,y_mask,):\n",
        "    for decodeLayer in self.decodeLayers:\n",
        "      x=decodeLayer(mem,mem_mask,y,y_mask)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sEU1yx7F3ix6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubLayerConnection(nn.Module):\n",
        "  def __init__(self,embed_size,dropout=0.01):\n",
        "    super(SubLayerConnection,self).__init__()\n",
        "    self.norm = LayerNorm(embed_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self,subModule,x):\n",
        "    return x+self.dropout(self.norm(subModule(x)))"
      ],
      "metadata": {
        "id": "S-L9ujzZaPrY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,size):\n",
        "    super(FeedForward,self).__init__()\n",
        "    self.ff1=nn.Linear(size,size)\n",
        "    self.ff2=nn.Linear(size,size)\n",
        "    self.relu=nn.ReLU()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x=self.relu(self.ff1(x))\n",
        "    return self.ff2(x)\n",
        "   \n"
      ],
      "metadata": {
        "id": "RTBUu43q1_P1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncodeLayer(nn.Module):\n",
        "  def __init__(self,head,d_model):\n",
        "    super(EncodeLayer,self).__init__()\n",
        "    self.multiHeadAtten= MultiHeadAttention(head,d_model)\n",
        "    self.feedForward=FeedForward(d_model)\n",
        "    self.subLayerCon=clone(SubLayerConnection(d_model),2)\n",
        "    \n",
        "\n",
        "  def forward(self,x,x_mask):\n",
        "    x=self.subLayerCon[0](lambda x: self.multiHeadAtten(x,x,x,x_mask),x)\n",
        "    x=self.subLayerCon[1](self.feedForward,x)\n",
        "    return x\n",
        "    "
      ],
      "metadata": {
        "id": "QPqe3zEObu5K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecodeLayer(nn.Module):\n",
        "  def __init__(self,head,d_model):\n",
        "    super(DecodeLayer,self).__init__()\n",
        "    self.multiHeadAtten1=MultiHeadAttention(head,d_model)\n",
        "    self.multiHeadAtten2=MultiHeadAttention(head,d_model)\n",
        "    self.feedForward=FeedForward(d_model)\n",
        "    self.subLayerCon=clone(SubLayerConnection(d_model),3)\n",
        "    \n",
        "\n",
        "  def forward(self,mem,mem_mask,tgt,tgt_mask):\n",
        "    # print(\"***********decoder\")\n",
        "   \n",
        "    x=self.subLayerCon[0](lambda tgt: self.multiHeadAtten1(tgt,tgt,tgt,tgt_mask),tgt)\n",
        "    # print(\"decoder::: MH---1 size of x  \"+str(x.size())+\"size of mem\"+str(mem.size()))\n",
        "    x=self.subLayerCon[1](lambda x: self.multiHeadAtten2(x,mem,mem,mem_mask),x)\n",
        "    # print(\"decoder::: MH---2 size of x  \"+str(x.size())+\"size of mem\"+str(mem.size()))\n",
        "\n",
        "    x=self.subLayerCon[2](self.feedForward,x)\n",
        "    # print(\"decoder::: FF---1 size of x  \"+str(x.size())+\"size of mem\"+str(mem.size()))\n",
        "    return x\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "6yHWY1Wwe3p2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,vocab_len,d_model):\n",
        "    super(Embedding,self).__init__()\n",
        "    self.embedding=nn.Embedding(vocab_len,d_model)\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)"
      ],
      "metadata": {
        "id": "PnUDop1ZvNvA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q=torch.rand(5,4)\n",
        "\n",
        "print(q)\n",
        "q.size()\n",
        "torch.triu(torch.ones(5,4))==0\n",
        "mask=torch.triu(torch.ones(5,4))==0\n",
        "q.masked_fill_(mask,10)\n",
        "q\n",
        "float('-inf')\n",
        "q@q.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5mdpaWe--yy",
        "outputId": "35e140a5-8a6e-437a-fb75-fa0e0d00f5e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1997, 0.0624, 0.0171, 0.7230],\n",
            "        [0.3298, 0.4054, 0.6958, 0.6581],\n",
            "        [0.8561, 0.5428, 0.3273, 0.9179],\n",
            "        [0.1285, 0.7261, 0.9062, 0.4912],\n",
            "        [0.7482, 0.4286, 0.1552, 0.2536]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.5668,   2.5098,   3.2898,   3.1468,  10.0218],\n",
              "        [  2.5098, 101.0817, 104.8861, 111.3359, 117.5939],\n",
              "        [  3.2898, 104.8861, 200.9498, 203.7244, 212.4529],\n",
              "        [  3.1468, 111.3359, 203.7244, 300.2413, 304.9118],\n",
              "        [ 10.0218, 117.5939, 212.4529, 304.9118, 400.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gLaCyp2tHHWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import NoneType\n",
        "# def attention(key,query,value,mask=None):\n",
        "#   '''\n",
        "#   key-size(-1,n,d_k)\n",
        "#   query-size(-1,n,d_q)\n",
        "#   value size(-1,n,d_v)\n",
        "#   d_k,d_v,d_q=d\n",
        "#   '''\n",
        "#   d=key.size(dim=-1)\n",
        "#   attent=torch.matmul(key,torch.transpose(query,-2,-1)) #size(n,n)\n",
        "#   attent=attent/torch.sqrt(d)#size(n,n)\n",
        "#   if mask is not None:\n",
        "#         attent = attent.masked_fill(mask == 0, -1e9)\n",
        "#   attent=attent.softMax(dim=-1)#size(n,n)\n",
        "\n",
        "#   attentValue= torch.matmul(attent,value)#size(n,d)\n",
        "#   return attent,attentValue\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,embed_size,d_k_size,d_v_size):\n",
        "    super(Attention,self).__init__()\n",
        "    self.k_lin=nn.Linear(embed_size,d_k_size)\n",
        "    self.q_lin=nn.Linear(embed_size,d_k_size)\n",
        "    self.v_lin=nn.Linear(embed_size,d_v_size)\n",
        "    self.d_k_size=d_k_size\n",
        "    self.d_v_size=d_v_size\n",
        "    \n",
        "  def forward(self,query,key,value,mask=None):\n",
        "    # breakpoint()\n",
        "    key=self.k_lin(key)#batch,seq,d_k\n",
        "    query=self.q_lin(query)#batch,seq,d_k\n",
        "    value=self.v_lin(value)#batch,seq,d_v\n",
        "    #breakpoint()\n",
        "    attent=torch.matmul(query,key.transpose(-1,-2))/math.sqrt(self.d_k_size)#batch,seq(query),seq(value)\n",
        "    if mask is not None:\n",
        "      # breakpoint()\n",
        "      attent.masked_fill_(mask,float('-inf'))\n",
        "    attent=attent.softmax(dim=-1)\n",
        "    return attent, attent@value\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#   def __init__(self,h,d_model,dropout=0.1):\n",
        "#     super(MultiHeadAttention,self).__init__()\n",
        "#     assert d_model%h==0\n",
        "#     self.d_k=d_model//h\n",
        "#     self.linears=clone(nn.Linear(d_model,self.d_k),3*h)\n",
        "#     self.atten=torch.zeros((0,self.d_k))#d_k should be \n",
        "#     self.attenValue=torch.zeros((0,self.d_k))\n",
        "    \n",
        "\n",
        "#   def forward(self,x,x_mask):\n",
        "#     head_count=0\n",
        "#     for i in range(0,len(self.linears),3):\n",
        "#       v=self.linear[i](x)\n",
        "#       k=self.linear[i+1](x)\n",
        "#       q=self.linear[i+2](x)\n",
        "#       attent,attentValue=attention(k,q,v,x_mask)\n",
        "\n",
        "#       head_count=head_count+1\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,h,d_model,dropout=0.1):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    assert d_model%h==0\n",
        "    self.d_k=d_model//h\n",
        "    self.d_v=d_model//h\n",
        "    self.d_model=d_model\n",
        "    self.attHeads=clone(Attention(d_model,self.d_k,self.d_v),h)\n",
        "\n",
        "  def forward(self,query,key,value,x_mask):\n",
        "    appendVal=torch.Tensor([])\n",
        "    \n",
        "    for i,attHead in enumerate(self.attHeads):\n",
        "      # breakpoint()\n",
        "      _,attentVal=attHead(query,key,value,x_mask)\n",
        "      appendVal=torch.cat((appendVal,attentVal),-1)\n",
        "    return appendVal\n",
        "    \n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "id": "uoxzyzlFhMcg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.01, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        print(div_term)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        print(div_term)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1sEQsv4ofD6a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=2\n",
        "seq_len=20\n",
        "d_model=8\n",
        "tgt_vocab_len=100\n",
        "src_vocab_len=100\n",
        "num_head=2\n",
        "num_layer=2\n",
        "\n",
        "src=torch.randint(0,100,(batch,seq_len))\n",
        "tgt=torch.randint(0,100,(batch,seq_len))\n",
        "tgt_mask=torch.triu(torch.ones(seq_len,seq_len))==0\n",
        "tgt_mask=torch.unsqueeze(tgt_mask,0)\n",
        "src_mask=torch.ones(seq_len,seq_len)\n",
        "src_mask=torch.unsqueeze(src_mask,0)\n",
        "\n",
        "\n",
        "tgt_mask=torch.zeros(1,1)\n",
        "print(\"tgt_mask size \"+str(tgt_mask.size()))\n",
        "print(\"tgt_mask size \"+str(tgt.size()))\n",
        "# tgt_mask=None\n",
        "src_mask=None\n",
        "# enc=EncodeLayer(2,16)\n",
        "# dec=DecodeLayer(2,16)\n",
        "enc=Encoder(num_layer,copy.deepcopy(EncodeLayer(num_head,d_model)))\n",
        "dec=Decoder(num_layer,copy.deepcopy(DecodeLayer(num_head,d_model)))\n",
        "# mem=enc(src,src_mask)\n",
        "# output=dec(mem,torch.ones(10,10),mask,torch.rand(2,10,16))\n",
        "# q=torch.Tensor([])\n",
        "# torch.cat((q,torch.ones(2,3)),0)\n",
        "# print(output.size())\n",
        "encDec=EncoderDecoder(enc,dec,src_vocab_len,tgt_vocab_len,d_model,Generator(d_model,tgt_vocab_len))\n",
        "# print(tgt.size())\n",
        "# print(src.size())\n",
        "\n",
        "# output=encDec(src,src_mask,src,tgt_mask)\n",
        "# output\n",
        "# print(\"tgt size\" +str(output.size()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6xnwKURHJI3",
        "outputId": "731a0b11-afb8-4a1c-d167-13d4e59b0a08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tgt_mask size torch.Size([1, 1])\n",
            "tgt_mask size torch.Size([2, 20])\n",
            "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])\n",
            "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(src[:,0].unsqueeze(1))\n",
        "print(src)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2UuCopjRJpY",
        "outputId": "5cc2c323-ae12-4336-e83b-3b64484e1116"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[34],\n",
            "        [51]])\n",
            "tensor([[34, 64, 79, 74, 78,  7, 93,  5, 22, 30, 50, 75, 83, 11, 61, 66,  7, 57,\n",
            "         40, 31],\n",
            "        [51, 97, 80, 39, 50, 18, 65, 33, 63, 63, 13, 97, 42, 25, 19, 20, 14, 18,\n",
            "         59, 19]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epoch=5\n",
        "optimizer = torch.optim.Adam(encDec.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion=nn.KLDivLoss()\n",
        "\n",
        "temp_tgt=torch.Tensor([])\n",
        "for j in range(epoch):\n",
        "  mem=encDec.encode(src,src_mask)\n",
        "  prev_tgt=None\n",
        "  for i in range(0,100):\n",
        "    # breakpoint()\n",
        "    cur_tgt= src[:,i].unsqueeze(1) if i==0 else prev_tgt\n",
        "    x=encDec.decode(mem,src_mask,cur_tgt,tgt_mask)\n",
        "    \n",
        "   \n",
        "\n",
        "  \n",
        "    # x=x.argmax(dim=-1)\n",
        "    # x=x.to(dtype=torch.long)\n",
        "    # tgt=tgt.to(dtype=torch.long)\n",
        "    # print(tgt)\n",
        "    # print(x)\n",
        "    # print(tgt)\n",
        "    # print(x.dtype)\n",
        "    # loss=criterion(x,tgt)\n",
        "    # print(loss)\n",
        "\n",
        "    # # print(x.size())\n",
        "    \n",
        "    # if i==0:\n",
        "    #   temp_gt=x\n",
        "    # else:\n",
        "    temp_tgt=torch.cat((temp_tgt,x), dim=1)\n",
        "    prev_tgt=x.argmax(dim=-1)\n",
        "    print(i,end=\"\\t \")\n",
        "  # loss.backward()\n",
        "  # optimizer.step()\n",
        "  # optimizer.zero_grad()\n",
        "  print(temp_tgt)\n",
        "  print(temp_tgt.size())\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AozZ2jE7gQ6L",
        "outputId": "9534bf2c-db51-469e-ae86-37d593492302"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\t 41\t 42\t 43\t 44\t 45\t 46\t 47\t 48\t 49\t 50\t 51\t 52\t 53\t 54\t 55\t 56\t 57\t 58\t 59\t 60\t 61\t 62\t 63\t 64\t 65\t 66\t 67\t 68\t 69\t 70\t 71\t 72\t 73\t 74\t 75\t 76\t 77\t 78\t 79\t 80\t 81\t 82\t 83\t 84\t 85\t 86\t 87\t 88\t 89\t 90\t 91\t 92\t 93\t 94\t 95\t 96\t 97\t 98\t 99\t tensor([[[-5.8558, -4.3622, -5.3884,  ..., -6.4972, -6.7916, -5.6623],\n",
            "         [-9.4304, -4.9201, -7.5596,  ..., -6.3291, -9.2094, -8.6107],\n",
            "         [-7.0424, -4.1201, -6.3251,  ..., -6.8691, -8.1339, -6.7436],\n",
            "         ...,\n",
            "         [-5.1190, -5.6316, -4.6005,  ..., -6.5624, -4.5872, -4.8078],\n",
            "         [-5.2490, -5.8104, -4.8644,  ..., -6.2485, -5.5750, -4.6060],\n",
            "         [-7.5246, -5.8836, -6.2460,  ..., -7.9293, -7.7560, -5.7562]],\n",
            "\n",
            "        [[-6.9114, -4.8192, -5.9367,  ..., -6.9095, -8.5123, -5.9523],\n",
            "         [-5.9950, -5.6567, -5.6282,  ..., -6.4413, -6.3068, -4.8624],\n",
            "         [-6.3343, -5.7042, -5.5343,  ..., -6.2661, -6.2132, -4.8707],\n",
            "         ...,\n",
            "         [-5.2616, -5.7758, -4.8278,  ..., -6.2464, -5.6042, -4.5879],\n",
            "         [-7.5302, -5.8589, -6.1310,  ..., -7.8567, -7.7583, -5.6779],\n",
            "         [-4.8958, -5.4542, -4.8480,  ..., -6.0273, -5.4908, -4.9508]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 100, 100])\n",
            "0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\t 41\t 42\t 43\t 44\t 45\t 46\t 47\t 48\t 49\t 50\t 51\t 52\t 53\t 54\t 55\t 56\t 57\t 58\t 59\t 60\t 61\t 62\t 63\t 64\t 65\t 66\t 67\t 68\t 69\t 70\t 71\t 72\t 73\t 74\t 75\t 76\t 77\t 78\t 79\t 80\t 81\t 82\t 83\t 84\t 85\t 86\t 87\t 88\t 89\t 90\t 91\t 92\t 93\t 94\t 95\t 96\t 97\t 98\t 99\t tensor([[[-5.8558, -4.3622, -5.3884,  ..., -6.4972, -6.7916, -5.6623],\n",
            "         [-9.4304, -4.9201, -7.5596,  ..., -6.3291, -9.2094, -8.6107],\n",
            "         [-7.0424, -4.1201, -6.3251,  ..., -6.8691, -8.1339, -6.7436],\n",
            "         ...,\n",
            "         [-5.2404, -5.8328, -4.8585,  ..., -6.2388, -5.5613, -4.5863],\n",
            "         [-7.5270, -5.8775, -6.2159,  ..., -7.9122, -7.7634, -5.7340],\n",
            "         [-5.8063, -5.3197, -5.5372,  ..., -6.2949, -6.3067, -5.5578]],\n",
            "\n",
            "        [[-6.9114, -4.8192, -5.9367,  ..., -6.9095, -8.5123, -5.9523],\n",
            "         [-5.9950, -5.6567, -5.6282,  ..., -6.4413, -6.3068, -4.8624],\n",
            "         [-6.3343, -5.7042, -5.5343,  ..., -6.2661, -6.2132, -4.8707],\n",
            "         ...,\n",
            "         [-5.9830, -5.6425, -5.6046,  ..., -6.4405, -6.2972, -4.8534],\n",
            "         [-5.9699, -5.6204, -5.5885,  ..., -6.4165, -6.2955, -4.8588],\n",
            "         [-5.9830, -5.6425, -5.6046,  ..., -6.4405, -6.2972, -4.8534]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 200, 100])\n",
            "0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\t 41\t 42\t 43\t 44\t 45\t 46\t 47\t 48\t 49\t 50\t 51\t 52\t 53\t 54\t 55\t 56\t 57\t 58\t 59\t 60\t 61\t 62\t 63\t 64\t 65\t 66\t 67\t 68\t 69\t 70\t 71\t 72\t 73\t 74\t 75\t 76\t 77\t 78\t 79\t 80\t 81\t 82\t 83\t 84\t 85\t 86\t 87\t 88\t 89\t 90\t 91\t 92\t 93\t 94\t 95\t 96\t 97\t 98\t 99\t tensor([[[-5.8558, -4.3622, -5.3884,  ..., -6.4972, -6.7916, -5.6623],\n",
            "         [-9.4304, -4.9201, -7.5596,  ..., -6.3291, -9.2094, -8.6107],\n",
            "         [-7.0424, -4.1201, -6.3251,  ..., -6.8691, -8.1339, -6.7436],\n",
            "         ...,\n",
            "         [-5.0320, -5.6127, -4.5387,  ..., -6.5297, -4.5791, -4.7877],\n",
            "         [-5.2283, -5.8632, -4.8469,  ..., -6.2241, -5.5424, -4.5558],\n",
            "         [-7.1543, -5.8351, -5.6905,  ..., -6.7394, -6.6103, -4.9032]],\n",
            "\n",
            "        [[-6.9114, -4.8192, -5.9367,  ..., -6.9095, -8.5123, -5.9523],\n",
            "         [-5.9950, -5.6567, -5.6282,  ..., -6.4413, -6.3068, -4.8624],\n",
            "         [-6.3343, -5.7042, -5.5343,  ..., -6.2661, -6.2132, -4.8707],\n",
            "         ...,\n",
            "         [-5.4455, -5.5536, -5.0125,  ..., -5.1251, -4.3522, -5.0758],\n",
            "         [-5.2668, -5.7559, -4.8037,  ..., -6.2445, -5.6176, -4.5750],\n",
            "         [-7.5308, -5.8558, -6.1210,  ..., -7.8507, -7.7556, -5.6731]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 300, 100])\n",
            "0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\t 41\t 42\t 43\t 44\t 45\t 46\t 47\t 48\t 49\t 50\t 51\t 52\t 53\t 54\t 55\t 56\t 57\t 58\t 59\t 60\t 61\t 62\t 63\t 64\t 65\t 66\t 67\t 68\t 69\t 70\t 71\t 72\t 73\t 74\t 75\t 76\t 77\t 78\t 79\t 80\t 81\t 82\t 83\t 84\t 85\t 86\t 87\t 88\t 89\t 90\t 91\t 92\t 93\t 94\t 95\t 96\t 97\t 98\t 99\t tensor([[[-5.8558, -4.3622, -5.3884,  ..., -6.4972, -6.7916, -5.6623],\n",
            "         [-9.4304, -4.9201, -7.5596,  ..., -6.3291, -9.2094, -8.6107],\n",
            "         [-7.0424, -4.1201, -6.3251,  ..., -6.8691, -8.1339, -6.7436],\n",
            "         ...,\n",
            "         [-5.1245, -5.6425, -4.5505,  ..., -6.4953, -4.5660, -4.7593],\n",
            "         [-5.2399, -5.8341, -4.8581,  ..., -6.2382, -5.5605, -4.5850],\n",
            "         [-7.5236, -5.8878, -6.2506,  ..., -7.9285, -7.7584, -5.7540]],\n",
            "\n",
            "        [[-6.9114, -4.8192, -5.9367,  ..., -6.9095, -8.5123, -5.9523],\n",
            "         [-5.9950, -5.6567, -5.6282,  ..., -6.4413, -6.3068, -4.8624],\n",
            "         [-6.3343, -5.7042, -5.5343,  ..., -6.2661, -6.2132, -4.8707],\n",
            "         ...,\n",
            "         [-5.2659, -5.7606, -4.8117,  ..., -6.2458, -5.6146, -4.5803],\n",
            "         [-7.5324, -5.8510, -6.1149,  ..., -7.8502, -7.7518, -5.6752],\n",
            "         [-5.8523, -5.2752, -5.5261,  ..., -6.2831, -6.3268, -5.5701]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 400, 100])\n",
            "0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\t 41\t 42\t 43\t 44\t 45\t 46\t 47\t 48\t 49\t 50\t 51\t 52\t 53\t 54\t 55\t 56\t 57\t 58\t 59\t 60\t 61\t 62\t 63\t 64\t 65\t 66\t 67\t 68\t 69\t 70\t 71\t 72\t 73\t 74\t 75\t 76\t 77\t 78\t 79\t 80\t 81\t 82\t 83\t 84\t 85\t 86\t 87\t 88\t 89\t 90\t 91\t 92\t 93\t 94\t 95\t 96\t 97\t 98\t 99\t tensor([[[-5.8558, -4.3622, -5.3884,  ..., -6.4972, -6.7916, -5.6623],\n",
            "         [-9.4304, -4.9201, -7.5596,  ..., -6.3291, -9.2094, -8.6107],\n",
            "         [-7.0424, -4.1201, -6.3251,  ..., -6.8691, -8.1339, -6.7436],\n",
            "         ...,\n",
            "         [-5.2267, -5.8668, -4.8515,  ..., -6.2249, -5.5389, -4.5586],\n",
            "         [-6.9448, -5.6743, -5.6434,  ..., -6.5687, -6.4358, -4.8917],\n",
            "         [-5.5001, -5.3026, -5.7693,  ..., -7.0944, -6.4888, -5.8402]],\n",
            "\n",
            "        [[-6.9114, -4.8192, -5.9367,  ..., -6.9095, -8.5123, -5.9523],\n",
            "         [-5.9950, -5.6567, -5.6282,  ..., -6.4413, -6.3068, -4.8624],\n",
            "         [-6.3343, -5.7042, -5.5343,  ..., -6.2661, -6.2132, -4.8707],\n",
            "         ...,\n",
            "         [-5.9894, -5.6543, -5.6085,  ..., -6.4407, -6.3129, -4.8449],\n",
            "         [-5.9894, -5.6543, -5.6085,  ..., -6.4407, -6.3129, -4.8449],\n",
            "         [-5.9894, -5.6543, -5.6085,  ..., -6.4407, -6.3129, -4.8449]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 500, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=torch.Tensor([])\n",
        "for i in range(10):\n",
        "  q=torch.cat((q,torch.randn(2,1,100)),dim=1)\n",
        "print(q.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTRmNSOeQFFb",
        "outputId": "0b09c210-ac61-46ee-9453-8ead3099f751"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[\n",
        "    [[1,0,0],[1,0,0]], # predict class 0 for pixel (0,0) and class 0 for pixel (0,1)\n",
        "    [[0,1,0],[0,0,1]], # predict class 1 for pixel (1,0) and class 2 for pixel (1,1)\n",
        "]])*5  # multiply by 5 to give bigger losses\n",
        "print(\"logits map :\")\n",
        "print(x)\n",
        "\n",
        "# ground truth labels\n",
        "y = np.array([[\n",
        "    [0,1], # must predict class 0 for pixel (0,0) and class 1 for pixel (0,1)\n",
        "    [1,2], # must predict class 1 for pixel (1,0) and class 2 for pixel (1,1)\n",
        "]])  \n",
        "print(\"\\nlabels map :\")\n",
        "print(y)\n",
        "\n",
        "x=torch.Tensor(x).permute((0,3,1,2))  # shape of preds must be (N, C, H, W) instead of (N, H, W, C)\n",
        "y=torch.Tensor(y).long() #  shape of labels must be (N, H, W) and type must be long integer\n",
        "\n",
        "\n",
        "print(x.size())\n",
        "print(y.size())\n",
        "print(x)\n",
        "print(y)\n",
        "losses = nn.CrossEntropyLoss(reduction=\"none\")(x, y)  # reduction=\"none\" to get the loss by pixel \n",
        "print(\"\\nLosses map :\")\n",
        "print(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjSf9uyJF4sb",
        "outputId": "ea0ac930-c7d2-4e4b-e5c1-0fc812d0bbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits map :\n",
            "[[[[5 0 0]\n",
            "   [5 0 0]]\n",
            "\n",
            "  [[0 5 0]\n",
            "   [0 0 5]]]]\n",
            "\n",
            "labels map :\n",
            "[[[0 1]\n",
            "  [1 2]]]\n",
            "torch.Size([1, 3, 2, 2])\n",
            "torch.Size([1, 2, 2])\n",
            "tensor([[[[5., 5.],\n",
            "          [0., 0.]],\n",
            "\n",
            "         [[0., 0.],\n",
            "          [5., 0.]],\n",
            "\n",
            "         [[0., 0.],\n",
            "          [0., 5.]]]])\n",
            "tensor([[[0, 1],\n",
            "         [1, 2]]])\n",
            "\n",
            "Losses map :\n",
            "tensor([[[0.0134, 5.0134],\n",
            "         [0.0134, 0.0134]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 3, requires_grad=True)\n",
        "target = torch.randn(3, 5).softmax(dim=1)\n",
        "output = loss(input, target)\n",
        "print(input.size())\n",
        "print(target.size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "mx4riubX2w-s",
        "outputId": "8fef3508-eb3f-426d-da63-a8a791a93f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-01ed98889ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.FloatTensor([[0.05, 0.9, 0.05],[0.05, 0.9, 0.05]])\n",
        "out = torch.autograd.Variable(out)\n",
        "\n",
        "# Categorical targets\n",
        "y = torch.LongTensor([1,2])\n",
        "y = torch.autograd.Variable(y)\n",
        "\n",
        "# One-hot encoded targets\n",
        "y1 = torch.FloatTensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
        "y1 = torch.autograd.Variable(y1)\n",
        "\n",
        "# Calculating the loss\n",
        "loss_val = nn.CrossEntropyLoss()(out, y)\n",
        "# loss_val1 = nn.BCEWithLogitsLoss()(out, y1)\n",
        "print(loss_val)\n",
        "print(out.size())\n",
        "print(y.size())\n",
        "\n",
        "tgt=torch.rand(3,3,5,100)\n",
        "x=torch.rand(3,3,5,100)\n",
        "criterion(x,tgt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7FUWEi2Muc4",
        "outputId": "2f51651b-96ff-4c72-d67a-4a12a99b3d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0428)\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6946)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}