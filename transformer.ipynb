{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhdgLqfiIMP8K6CxgQWRvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praburocking/transformers/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "khxo2YoFIxn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d358704e-4175-43ca-e416-e1b03dc0e967"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 717 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AG_yyBN_NYxu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "#import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.nn import LayerNorm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_interactive_notebook():\n",
        "    return __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "\n",
        "def execute_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        fn(*args)\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def __init__(self):\n",
        "        self.param_groups = [{\"lr\": 0}]\n",
        "        None\n",
        "\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        None\n",
        "\n",
        "\n",
        "class DummyScheduler:\n",
        "    def step(self):\n",
        "        None"
      ],
      "metadata": {
        "id": "eoERxGDKPK0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  first encoderDecoder (standard one)\n",
        "  \"\"\"\n",
        "  def __init__(self,encoder,decoder,src_vocab_len,tgt_vocab_len,d_model,generator):\n",
        "    super(EncoderDecoder,self).__init__()\n",
        "    \n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=nn.Embedding(src_vocab_len,d_model)\n",
        "    self.tgt_embed=nn.Embedding(tgt_vocab_len,d_model)\n",
        "    self.pos=PositionalEncoding(d_model)\n",
        "    self.gen=generator\n",
        "    \n",
        "  def forward(self,src,src_mask,tgt,tgt_mask):\n",
        "    return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  def encode(self,src,mask_src):\n",
        "    # breakpoint()\n",
        "    return self.encoder(self.pos(self.src_embed(src)),mask_src)\n",
        "\n",
        "  def decode(self,mem,mem_mask,tgt,tgt_mask):\n",
        "    # breakpoint()\n",
        "    return self.gen(self.decoder(mem,mem_mask,self.pos(self.tgt_embed(tgt)),tgt_mask))"
      ],
      "metadata": {
        "id": "okwfYb5ePTi-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super(Generator,self).__init__()\n",
        "    self.linear=nn.Linear(d_model,vocab_size)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return log_softmax(self.linear(x),dim=-1)"
      ],
      "metadata": {
        "id": "VTWZJQPodp3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "z_3cm87qtsRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,N,module):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.encodeLayers=clone(module,N)\n",
        "      \n",
        "  def forward(self,x,mask):\n",
        "    for encodeLayer in self.encodeLayers:\n",
        "      x=encodeLayer(x,mask)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,N,module):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.decodeLayers=clone(module,N)\n",
        "    \n",
        "  def forward(self,mem,mem_mask,y,y_mask,):\n",
        "    for decodeLayer in self.decodeLayers:\n",
        "      x=decodeLayer(mem,mem_mask,y,y_mask)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sEU1yx7F3ix6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubLayerConnection(nn.Module):\n",
        "  def __init__(self,embed_size,dropout=0.01):\n",
        "    super(SubLayerConnection,self).__init__()\n",
        "    self.norm = LayerNorm(embed_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self,subModule,x):\n",
        "    return x+self.dropout(self.norm(subModule(x)))"
      ],
      "metadata": {
        "id": "S-L9ujzZaPrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,size):\n",
        "    super(FeedForward,self).__init__()\n",
        "    self.ff1=nn.Linear(size,size)\n",
        "    self.ff2=nn.Linear(size,size)\n",
        "    self.relu=nn.ReLU()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x=self.relu(self.ff1(x))\n",
        "    return self.ff2(x)\n",
        "   \n"
      ],
      "metadata": {
        "id": "RTBUu43q1_P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncodeLayer(nn.Module):\n",
        "  def __init__(self,head,d_model):\n",
        "    super(EncodeLayer,self).__init__()\n",
        "    self.multiHeadAtten= MultiHeadAttention(head,d_model)\n",
        "    self.feedForward=FeedForward(d_model)\n",
        "    self.subLayerCon=clone(SubLayerConnection(d_model),2)\n",
        "    \n",
        "\n",
        "  def forward(self,x,x_mask):\n",
        "    x=self.subLayerCon[0](lambda x: self.multiHeadAtten(x,x,x,x_mask),x)\n",
        "    x=self.subLayerCon[1](self.feedForward,x)\n",
        "    return x\n",
        "    "
      ],
      "metadata": {
        "id": "QPqe3zEObu5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecodeLayer(nn.Module):\n",
        "  def __init__(self,head,d_model):\n",
        "    super(DecodeLayer,self).__init__()\n",
        "    self.multiHeadAtten1=MultiHeadAttention(head,d_model)\n",
        "    self.multiHeadAtten2=MultiHeadAttention(head,d_model)\n",
        "    self.feedForward=FeedForward(d_model)\n",
        "    self.subLayerCon=clone(SubLayerConnection(d_model),3)\n",
        "    \n",
        "\n",
        "  def forward(self,mem,mem_mask,tgt,tgt_mask):\n",
        "    # breakpoint()\n",
        "    x=self.subLayerCon[0](lambda tgt: self.multiHeadAtten1(tgt,tgt,tgt,tgt_mask),tgt)\n",
        "    x=self.subLayerCon[1](lambda mem: self.multiHeadAtten2(x,mem,mem,mem_mask),mem)\n",
        "    x=self.subLayerCon[2](self.feedForward,x)\n",
        "    return x\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "6yHWY1Wwe3p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,vocab_len,d_model):\n",
        "    super(Embedding,self).__init__()\n",
        "    self.embedding=nn.Embedding(vocab_len,d_model)\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)"
      ],
      "metadata": {
        "id": "PnUDop1ZvNvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q=torch.rand(5,4)\n",
        "\n",
        "print(q)\n",
        "q.size()\n",
        "torch.triu(torch.ones(5,4))==0\n",
        "mask=torch.triu(torch.ones(5,4))==0\n",
        "q.masked_fill_(mask,10)\n",
        "q\n",
        "float('-inf')\n",
        "q@q.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5mdpaWe--yy",
        "outputId": "a68452a1-cc7d-4257-9ecf-cdd51df7c1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9778, 0.1773, 0.7058, 0.5408],\n",
            "        [0.6097, 0.4162, 0.2337, 0.0357],\n",
            "        [0.3823, 0.0247, 0.6154, 0.8204],\n",
            "        [0.5381, 0.7503, 0.6415, 0.3229],\n",
            "        [0.6275, 0.6348, 0.4910, 0.7243]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1.7782,  10.0360,  12.4294,  18.7839,  24.0177],\n",
              "        [ 10.0360, 100.2291, 104.3353, 106.5104, 106.8556],\n",
              "        [ 12.4294, 104.3353, 201.0518, 206.4186, 214.3582],\n",
              "        [ 18.7839, 106.5104, 206.4186, 300.1043, 303.2293],\n",
              "        [ 24.0177, 106.8556, 214.3582, 303.2293, 400.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gLaCyp2tHHWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import NoneType\n",
        "# def attention(key,query,value,mask=None):\n",
        "#   '''\n",
        "#   key-size(-1,n,d_k)\n",
        "#   query-size(-1,n,d_q)\n",
        "#   value size(-1,n,d_v)\n",
        "#   d_k,d_v,d_q=d\n",
        "#   '''\n",
        "#   d=key.size(dim=-1)\n",
        "#   attent=torch.matmul(key,torch.transpose(query,-2,-1)) #size(n,n)\n",
        "#   attent=attent/torch.sqrt(d)#size(n,n)\n",
        "#   if mask is not None:\n",
        "#         attent = attent.masked_fill(mask == 0, -1e9)\n",
        "#   attent=attent.softMax(dim=-1)#size(n,n)\n",
        "\n",
        "#   attentValue= torch.matmul(attent,value)#size(n,d)\n",
        "#   return attent,attentValue\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,embed_size,d_k_size,d_v_size):\n",
        "    super(Attention,self).__init__()\n",
        "    self.k_lin=nn.Linear(embed_size,d_k_size)\n",
        "    self.q_lin=nn.Linear(embed_size,d_k_size)\n",
        "    self.v_lin=nn.Linear(embed_size,d_v_size)\n",
        "    self.d_k_size=d_k_size\n",
        "    self.d_v_size=d_v_size\n",
        "    \n",
        "  def forward(self,query,key,value,mask=None):\n",
        "    # breakpoint()\n",
        "    key=self.k_lin(key)#batch,seq,d_k\n",
        "    query=self.q_lin(query)#batch,seq,d_k\n",
        "    value=self.v_lin(value)#batch,seq,d_v\n",
        "    # breakpoint()\n",
        "    attent=torch.matmul(query,key.transpose(-1,-2))/math.sqrt(self.d_k_size)#batch,seq(query),seq(value)\n",
        "    if mask is not None:\n",
        "      # breakpoint()\n",
        "      attent.masked_fill_(mask,float('-inf'))\n",
        "    attent=attent.softmax(dim=-1)\n",
        "    return attent, attent@value\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#   def __init__(self,h,d_model,dropout=0.1):\n",
        "#     super(MultiHeadAttention,self).__init__()\n",
        "#     assert d_model%h==0\n",
        "#     self.d_k=d_model//h\n",
        "#     self.linears=clone(nn.Linear(d_model,self.d_k),3*h)\n",
        "#     self.atten=torch.zeros((0,self.d_k))#d_k should be \n",
        "#     self.attenValue=torch.zeros((0,self.d_k))\n",
        "    \n",
        "\n",
        "#   def forward(self,x,x_mask):\n",
        "#     head_count=0\n",
        "#     for i in range(0,len(self.linears),3):\n",
        "#       v=self.linear[i](x)\n",
        "#       k=self.linear[i+1](x)\n",
        "#       q=self.linear[i+2](x)\n",
        "#       attent,attentValue=attention(k,q,v,x_mask)\n",
        "\n",
        "#       head_count=head_count+1\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,h,d_model,dropout=0.1):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    assert d_model%h==0\n",
        "    self.d_k=d_model//h\n",
        "    self.d_v=d_model//h\n",
        "    self.d_model=d_model\n",
        "    self.attHeads=clone(Attention(d_model,self.d_k,self.d_v),h)\n",
        "\n",
        "  def forward(self,query,key,value,x_mask):\n",
        "    appendVal=torch.Tensor([])\n",
        "    \n",
        "    for i,attHead in enumerate(self.attHeads):\n",
        "      # breakpoint()\n",
        "      _,attentVal=attHead(query,key,value,x_mask)\n",
        "      appendVal=torch.cat((appendVal,attentVal),-1)\n",
        "    return appendVal\n",
        "    \n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "id": "uoxzyzlFhMcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.01, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        print(div_term)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        print(div_term)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1sEQsv4ofD6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=2\n",
        "seq_len=20\n",
        "d_model=8\n",
        "tgt_vocab_len=100\n",
        "src_vocab_len=100\n",
        "num_head=2\n",
        "num_layer=2\n",
        "\n",
        "src=torch.randint(0,99,(batch,seq_len))\n",
        "tgt=torch.randint(0,99,(batch,seq_len))\n",
        "tgt_mask=torch.triu(torch.ones(seq_len,seq_len))==0\n",
        "tgt_mask=torch.unsqueeze(tgt_mask,0)\n",
        "src_mask=torch.ones(seq_len,seq_len)\n",
        "src_mask=torch.unsqueeze(src_mask,0)\n",
        "\n",
        "\n",
        "# enc=EncodeLayer(2,16)\n",
        "# dec=DecodeLayer(2,16)\n",
        "enc=Encoder(num_layer,copy.deepcopy(EncodeLayer(num_head,d_model)))\n",
        "dec=Decoder(8,copy.deepcopy(DecodeLayer(num_head,d_model)))\n",
        "# mem=enc(src,src_mask)\n",
        "# output=dec(mem,torch.ones(10,10),mask,torch.rand(2,10,16))\n",
        "# q=torch.Tensor([])\n",
        "# torch.cat((q,torch.ones(2,3)),0)\n",
        "# print(output.size())\n",
        "encDec=EncoderDecoder(enc,dec,src_vocab_len,tgt_vocab_len,d_model,Generator(d_model,tgt_vocab_len))\n",
        "print(tgt.size())\n",
        "print(src.size())\n",
        "output=encDec(src,src_mask,tgt,tgt_mask)\n",
        "output.ma\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6xnwKURHJI3",
        "outputId": "f9a2908a-10ba-4764-fca5-a000189f6b71"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])\n",
            "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])\n",
            "torch.Size([2, 20])\n",
            "torch.Size([2, 20])\n",
            "torch.Size([2, 20, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.tensor([[2,3,4],[2,3,4]]).size()\n",
        "# lin=nn.Linear(5,3)\n",
        "# a=torch.rand((3,4,5))\n",
        "# lin(a)\n",
        "# lin.weight.shape\n",
        "# a=torch.Tensor([[]])\n",
        "# a[0,:]=torch.Tensor([1,2,3,4])\n",
        "# a\n",
        "\n",
        "q=torch.rand(2,10,10)\n",
        "print(output.max(dim=-1))\n",
        "print(src)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AozZ2jE7gQ6L",
        "outputId": "47a8a3c6-ad32-4fcd-ba0c-e92346359ae9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([[-1.7748, -2.1283, -1.1489, -2.3102, -1.5027, -1.5435, -2.5304, -1.3228,\n",
            "         -1.8757, -1.2773, -1.5106, -1.8929, -2.2803, -2.0485, -1.5822, -2.0528,\n",
            "         -1.7103, -2.1180, -1.8704, -2.0196],\n",
            "        [-2.1902, -1.9152, -1.3068, -2.3500, -3.0056, -1.8908, -2.4926, -2.2137,\n",
            "         -1.5956, -1.8084, -2.2959, -1.3956, -2.1741, -2.6560, -2.5848, -2.7051,\n",
            "         -2.1005, -1.4640, -2.6729, -2.4243]], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([[87, 87, 87, 87, 87, 87, 27, 87, 87, 22, 87, 27, 87, 27, 27, 87, 87, 22,\n",
            "         27, 37],\n",
            "        [27, 87, 87, 27, 36, 27, 36, 36, 87, 27, 57, 27, 87, 87, 87, 27, 37, 87,\n",
            "         87, 87]]))\n",
            "tensor([[23, 91, 23, 30, 86, 90, 11, 60, 50, 20, 59, 67, 52, 80, 42, 74, 94, 46,\n",
            "         90, 24],\n",
            "        [44, 89, 60, 84, 51, 65, 26, 34, 13, 54, 83, 77, 56, 61, 27, 26, 74, 66,\n",
            "         33, 87]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.Tensor([[1,2,3,4]])\n",
        "torch.cat((a,torch.Tensor([[1,2,3,4]])),0)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4riubX2w-s",
        "outputId": "9bcb9a80-6913-4083-fa9b-9bc1ed5cf061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}